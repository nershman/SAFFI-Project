{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#requires gensim 3.8.3 , NOT version 4: no wrapper for sklearn\n",
    "#python -m pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Term Counting Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if jupyternotify is installed, we can add %notify to a cell to get an alert when it ifnished running\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import metrics_helpers as indicators\n",
    "import pickle as pk\n",
    "import gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import traceback #needed to store full error tracebacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def dt_to_int(dt): #datetime to integer\n",
    "    return dt.astype('int')/(10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('/Users/sma/Documents/INRAE internship/scrape-git/facebook/untypod_dict.pkl', 'rb') as f:\n",
    "    netmums = pk.load(f)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "nm_ind = indicators.indicators(netmums, fb=False)\n",
    "#this one takes long, around 20 seconds I think.\n",
    "\n",
    "posts_dict = nm_ind.get_posts_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "hazards = {\n",
    "'Chemical contaminants': [],\n",
    "'Endocrine disruptor': [\"endocrine\",\"estrogen\"],\n",
    "'FOOD PRESERVATIVES, SWEETENERS AND ADDITIVES':[\"preservatives\",\"sweeteners\",\"additives\"],\n",
    "\"Pesticides\":[],\n",
    "\"Veterinary drugs\":[\"animal drugs\",\"vet drugs\"],\n",
    "'GMO':['GM',\"genetically modified\"],\n",
    "\"Metals\":[],\n",
    "\"Mycotoxin\":[],\n",
    "\"Bisphenol A\":['BPA','Bisphenol','BisphenolA'],\n",
    "'Furan':[],\n",
    "'DON': #(note that this acronym nobody uses and all results are from words like \"don't\")\n",
    "[\"deoxynivalenol\",\n",
    "\"vomitoxin\"],\n",
    "'DIOXIN AND PCB':[\"Dioxin\",\"PCB\",\"biphenyls\"],\n",
    "'MOSH and MOAH':[\"hydrocarbons\",\"saturated hydrocarbons\",\"MOAH\", 'MOH',\"aromatic hydrocarbons\"],\n",
    "'Nitrates':[],\n",
    "\"Acrylamid\":[\"Acrylamide\"],\n",
    "\"phthalates\":[],\n",
    "\"Microbiologic contaminants\":\n",
    "[\"spores\",\n",
    "\"mold\",\n",
    "\"mould\",\n",
    "#\"virus\",\n",
    "\"microbes\",\n",
    "\"contaminated\"],\n",
    "\"Salmonella\":[],\n",
    "\"Campylobacter\":[],\n",
    "\"Listeria\":[],\n",
    "\"EColi\":[\"E-coli\"],\n",
    "\"Cronobacter\":[],\n",
    "\"Histamine\":[],\n",
    "'other bacteria':[\"bacteria\"],\n",
    "\"Virus\":[],\n",
    "\"Parasites\":[],\n",
    "'Related Terms':[\"carcinogen\",\"chemicals\", \"toxic\", \"toxin\", \"poisonous\", \"fungus\", \"food poisoning\", \"hazard\",\"EFSA\",\"European Food Safety Authority\"]\n",
    "}\n",
    "\n",
    "products = {\n",
    "'infant formula':\n",
    "[\"formula\",\"baby formula\", \"bottle-fed\", \"bottle\"]\n",
    ",'sterilized vegetable mixed with fish':\n",
    "[\"veggie baby food\",\"vegetable baby food\",\n",
    "\"veg puree\", \"veg purée\"]\n",
    ",'fresh fruit puree mildly processed':\n",
    "[\"fruit puree\",\"fruit baby food\", \"fruit purée\", \"applesauce\", \"apple sauce\", \"fruit sauce\"]\n",
    ",'infant cereals':\n",
    "[\"cereal for baby\", \"cereal\", \"porridge\", \"oats\", \"oatmeal\"]\n",
    ",'other':\n",
    "[\"jar food\", \"baby food\", \"jarred\", \"premade food\", \"puree\", \"purée\", \"jarred food\"\n",
    ",\"yoghurt\", \"pudding\"]\n",
    "}\n",
    "\n",
    "\n",
    "#IMPORTANT!: terms used for count vectorizer must be lower-case o.w. get 0 matches\n",
    "hazards = {key.lower():[v.lower() for v in value] + [key.lower()] for key,value in hazards.items()}\n",
    "products = {key.lower():[v.lower() for v in value]+[key.lower()] for key,value in products.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "extras = {\\\n",
    "'baby_food_brands':\n",
    "['ellas',\n",
    "'organix',\n",
    "'heinz baby',\n",
    "\"plum baby\",\n",
    "'little angels',\n",
    "'farleys'],\n",
    "'formula_brands':['sma','aptamil comfort','infasoy','nutramigen','neocate','powdered milk','comfort milk'],\n",
    " 'food_or_formula_brands':\n",
    "['aptamil', # formula and cereals.\n",
    "'hipp organic',# - formula and baby food\n",
    "'cow gate','cow and gate','c g',\n",
    "'mamia'],\n",
    "##NON BRAND SIGNALS##\n",
    "'cereal':['baby_cereal','baby riceporridge','baby rice','baby porridge'],\n",
    "'baby_food':['mashed','tinned','premade','canned','jarred','pouches','pouch','ready made','readymade','cartons'],  \n",
    "#INDICATORS TO BE USED IN CONJUNCTION WITH 'baby food' label: this way we \n",
    "#can observe if both terms are used in a document (but are not used right next to each other.)\n",
    "'fruit':['fruit'],\n",
    "'vegetable':['vegetable'],\n",
    "'baby':['infant', 'baby' ,'for littles']\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def make_phrases(list_of_phrases, text):\n",
    "    \"\"\"\n",
    "    convert phrases to bigrams within a larger text corpus.\n",
    "    example: \"I love collard greens for breakfast\" -> \"I love collard_greens for breakfast\"\n",
    "    example: \"I love collard-greens for breakfast\" -> \"I love collard_greens for breakfast\"\n",
    "    \"\"\"\n",
    "    for phrase in list_of_phrases:\n",
    "        #spaces\n",
    "        text = re.sub(phrase, re.sub(' ', '_',phrase), text)\n",
    "        #hyphens\n",
    "        text = re.sub(re.sub(' ', '-', phrase), re.sub(' ', '_',phrase), text)\n",
    "    return text\n",
    "\n",
    "def make_underscores(item):\n",
    "    \"\"\"\n",
    "    recursively replace spaces and hyphens in strings, lists, sets, or other iterables.\n",
    "    Return the same type if string, list, set. If other type, returns list.\n",
    "    \"\"\"\n",
    "    if type(item) is str:\n",
    "        return re.sub(' |-', '_', item)\n",
    "    else:\n",
    "        temp = []\n",
    "        for thing in item:\n",
    "            temp.append(make_underscores(thing))\n",
    "    if type(item) is set:\n",
    "        return set(temp)\n",
    "    elif type(item) is list:\n",
    "        return temp\n",
    "    elif isinstance(item, type({}.keys())):\n",
    "        #if the object is a dict.key() view\n",
    "        return temp\n",
    "    else:\n",
    "        print('Object must be string, list, set, or dict.keys()')\n",
    "    #TODO this would be cleaner if i just check that it's iterable, and then check that it's a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#from the dict which representes our subcategories, create lists of all words in the subcategories.\n",
    "h = [item for val in hazards.values() for item in val]\n",
    "p = [item for val in products.values() for item in val]\n",
    "e = [item for val in extras.values() for item in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#concatenate list of all phrases (bigrams, anything with a space in it)\n",
    "phrases = {'baby formula', 'baby cereal'}.union({item for item in p + h + e if ' ' in item})\n",
    "\n",
    "#step 1: make a dict of just the text\n",
    "text_dict = {key:value['body'] for key,value in posts_dict.items()}\n",
    "\n",
    "#step 2 : convert the relevant phrases to bigrams with re.sub\n",
    "text_dict = {key: make_phrases(phrases, text) for key, text in text_dict.items()}\n",
    "\n",
    "#replace \"don't\" with \"do not\" (so that we don't get false positives for don count.)\n",
    "for key in text_dict:\n",
    "    text_dict[key] = re.sub('don[\\W]+t', 'do not', text_dict[key], flags=re.I) #TODO. there are cases of \"don' \" need to catch.\n",
    "\n",
    "#step 3: count occurences using countvectorizer\n",
    "vocab = p + h + e\n",
    "vocab = [re.sub(' |-','_',item) for item in vocab] #should I use make_underscores instead??\n",
    "vocab = set(vocab)\n",
    "term_counter = CountVectorizer(vocabulary = vocab, stop_words = 'english')\n",
    "counts = term_counter.fit_transform(text_dict.values())\n",
    "\n",
    "#note that hyphens will be treated as spaces by countvectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "counts = counts.toarray() #run once\n",
    "count_dict = {}\n",
    "for num, key in enumerate(text_dict.keys()): #TODO just use netmums, not text_dict?? its confusing. (they have the same keys)\n",
    "    count_dict[key] = {term: counts[num][value] for term, value in term_counter.vocabulary_.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "countdf = pd.DataFrame.from_dict(count_dict).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "summed_df = pd.DataFrame()\n",
    "\n",
    "for key in products.keys():\n",
    "    summed_df[key] = countdf[make_underscores(products[key])].sum(axis=1)\n",
    "for key in hazards.keys():\n",
    "    summed_df[key] = countdf[make_underscores(hazards[key])].sum(axis=1)\n",
    "for key in extras.keys():\n",
    "    summed_df[key] = countdf[make_underscores(extras[key])].sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#count mentions of fruit or vegetable\n",
    "#return 0 if there is no words indicating a context of BABY foods (not adult foods)\n",
    "#note that baby food brand names occur much more than fruit or veg. Am not sure if they co-occur.\n",
    "#TODO: maybe it is better to add the brands in with the fruit / veg. But since it is highly corr with them alreayd, at least looking by post it isnt a problem\n",
    "summed_df['fruit_in_baby_context'] = summed_df['fruit']  * (summed_df[['baby_food_brands', 'food_or_formula_brands', 'baby']].sum(axis=1) > 0)\n",
    "summed_df['veg_in_baby_context'] = summed_df['vegetable']  * (summed_df[['baby_food_brands', 'food_or_formula_brands', 'baby']].sum(axis=1) > 0)\n",
    "\n",
    "#if there is mention of fruit or vegetable it's not uncategorized. return 0\n",
    "# if no mentions, sum the counts of mentions of baby food brands\n",
    "# possible improvement: check for words indicating a food, or in weaning forum etc. THEN we can also add food_or_formula_brands to the COUNT.\n",
    "summed_df['baby_food_uncategorized'] = (summed_df[['fruit','vegetable']].sum(axis=1) > 0) * summed_df['baby_food_brands']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class_df = summed_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "product_cols = list(products.keys()) + ['veg_in_baby_context', 'fruit_in_baby_context', 'baby_food_uncategorized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#classify\n",
    "class_df['product_type'] = class_df[product_cols].idxmax(axis=1)\n",
    "# idxmax has a strange behavior where it will set all-zero sets to an arbitrary category (the first one available?)\n",
    "# so we must manually change them to an NA category.\n",
    "class_df.loc[class_df[product_cols].max(axis=1) == 0,'product_type'] = 'NA'\n",
    "# convert to categorical (factors)\n",
    "class_df['product_type'] = class_df['product_type'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#make classification for hazards and check it as well.\n",
    "class_df['hazard_type'] = class_df[hazards.keys()].idxmax(axis=1)\n",
    "class_df.loc[class_df[hazards.keys()].max(axis=1) == 0,'hazard_type'] = 'NA'\n",
    "class_df['hazard_type'] = class_df['hazard_type'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running KNN on the processed numbers. (maybe) TODO "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Guided LDA Approach..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " https://github.com/scign/GuidedLDA/blob/master/Guided%20LDA%20using%20gensim.ipynb\n",
    " \n",
    " how many possible cateorgires do we have?\n",
    " 4 Products\n",
    " 18 Hazards\n",
    "\n",
    " so we do\n",
    "     * k = 4\n",
    "     * k = 18\n",
    "     * k = $18*4$ = 72\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sma/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/sma/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify Penn tags to n (NOUN), v (VERB), a (ADJECTIVE) or r (ADVERB)\n",
    "def simplify(penn_tag):\n",
    "    pre = penn_tag[0]\n",
    "    if (pre == 'J'):\n",
    "        return 'a'\n",
    "    elif (pre == 'R'):\n",
    "        return 'r'\n",
    "    elif (pre == 'V'):\n",
    "        return 'v'\n",
    "    else:\n",
    "        return 'n'\n",
    "def preprocess(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    toks = gensim.utils.simple_preprocess(str(text), deacc=True)\n",
    "    wn = WordNetLemmatizer()\n",
    "    return [wn.lemmatize(tok, simplify(pos)) for tok, pos in nltk.pos_tag(toks) if tok not in stop_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = [preprocess(line) for line in text_dict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19671"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = gensim.corpora.Dictionary(corp)\n",
    "len(corp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lda model:\n",
    "# update_every - 0 is single-batch, 1 is online. any other number is the batch size\n",
    "# distributed - distributed computing (not relevant to us)\n",
    "# ns_conf - only used with distributed\n",
    "# \n",
    "# lda multicore:\n",
    "# batch - batch true or false\n",
    "# workers - \n",
    "# per_word_topics - proba of topic is assigned to each word. \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   model = gensim.models.ldamodel.LdaModel(\n",
    "#               corpus=bow, id2word=dictionary, num_topics=ntopics,\n",
    "#               random_state=42, chunksize=100, eta=eta,\n",
    "#               eval_every=-1, update_every=0, #0 is batch, 1 is online.\n",
    "#               passes=10, alpha='auto', per_word_topics=False) #I LOWERED THE PASSES BY A LOT TO BE FASTER, hEh.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://radimrehurek.com/gensim/models/ldamodel.html\n",
    "# https://radimrehurek.com/gensim/models/ldamulticore.html\n",
    "\n",
    "def test_eta(eta, dictionary, ntopics, print_topics=True, print_dist=True, the_corpus = corp ):\n",
    "    \"\"\"\n",
    "    Run a LDA model using the eta function.\n",
    "    \"\"\"\n",
    "    np.random.seed(42) # set the random seed for repeatability\n",
    "\n",
    "    bow = [dictionary.doc2bow(line) for line in the_corpus] # get the bow-format lines with the set dictionary\n",
    "    \n",
    "    with (np.errstate(divide='ignore')):  # ignore divide-by-zero warnings\n",
    "        model = gensim.models.ldamulticore.LdaMulticore(\n",
    "            corpus=bow, id2word=dictionary, num_topics=ntopics,\n",
    "            random_state=42, chunksize=100, eta=eta,\n",
    "            eval_every=-1, batch=True, #update_every=0, #0 is batch, 1 is online.\n",
    "            passes=10, alpha='symmetric', per_word_topics=False) #I LOWERED THE PASSES BY A LOT TO BE FASTER, hEh..\n",
    "        \n",
    "    # visuzlize the model term topics\n",
    "    print('Perplexity: {:.2f}'.format(model.log_perplexity(bow)))\n",
    "    if print_topics:\n",
    "        # display the top terms for each topic\n",
    "        for topic in range(ntopics):\n",
    "            print('Topic {}: {}'.format(topic, [dictionary[w] for w,p in model.get_topic_terms(topic, topn=18)]))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eta(priors, etadict, ntopics):\n",
    "    \"\"\"\n",
    "    Generates a matrix of coefficients corresponding to apriori-beliefs about word occurence in each class.\n",
    "    \"\"\"\n",
    "    eta = np.full(shape=(ntopics, len(etadict)), fill_value=1) # create a (ntopics, nterms) matrix and fill with 1\n",
    "    for word, topic in priors.items(): # for each word in the list of priors\n",
    "        keyindex = [index for index,term in etadict.items() if term==word] # look up the word in the dictionary\n",
    "        if (len(keyindex)>0): # if it's in the dictionary\n",
    "            eta[topic,keyindex[0]] = 1e7  # put a large number in there\n",
    "    eta = np.divide(eta, eta.sum(axis=0)) # normalize so that the probabilities sum to 1 over all topics\n",
    "    return eta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create eta (to me this is the topic which each word goes to... idk lol)\n",
    "\n",
    "apriori_hazards = {}\n",
    "apriori_products = {}\n",
    "for ind, key in enumerate(hazards.keys()):\n",
    "    for item in hazards[key]:\n",
    "        apriori_hazards[item] = ind\n",
    "        \n",
    "for ind, key in enumerate(products.keys()):\n",
    "    for item in products[key]:\n",
    "        apriori_products[item] = ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chemical contaminants': 0,\n",
       " 'endocrine': 1,\n",
       " 'estrogen': 1,\n",
       " 'endocrine disruptor': 1,\n",
       " 'preservatives': 2,\n",
       " 'sweeteners': 2,\n",
       " 'additives': 2,\n",
       " 'food preservatives, sweeteners and additives': 2,\n",
       " 'pesticides': 3,\n",
       " 'animal drugs': 4,\n",
       " 'vet drugs': 4,\n",
       " 'veterinary drugs': 4,\n",
       " 'gm': 5,\n",
       " 'genetically modified': 5,\n",
       " 'gmo': 5,\n",
       " 'metals': 6,\n",
       " 'mycotoxin': 7,\n",
       " 'bpa': 8,\n",
       " 'bisphenol': 8,\n",
       " 'bisphenola': 8,\n",
       " 'bisphenol a': 8,\n",
       " 'furan': 9,\n",
       " 'deoxynivalenol': 10,\n",
       " 'vomitoxin': 10,\n",
       " 'don': 10,\n",
       " 'dioxin': 11,\n",
       " 'pcb': 11,\n",
       " 'biphenyls': 11,\n",
       " 'dioxin and pcb': 11,\n",
       " 'hydrocarbons': 12,\n",
       " 'saturated hydrocarbons': 12,\n",
       " 'moah': 12,\n",
       " 'moh': 12,\n",
       " 'aromatic hydrocarbons': 12,\n",
       " 'mosh and moah': 12,\n",
       " 'nitrates': 13,\n",
       " 'acrylamide': 14,\n",
       " 'acrylamid': 14,\n",
       " 'phthalates': 15,\n",
       " 'spores': 16,\n",
       " 'mold': 16,\n",
       " 'mould': 16,\n",
       " 'microbes': 16,\n",
       " 'contaminated': 16,\n",
       " 'microbiologic contaminants': 16,\n",
       " 'salmonella': 17,\n",
       " 'campylobacter': 18,\n",
       " 'listeria': 19,\n",
       " 'e-coli': 20,\n",
       " 'ecoli': 20,\n",
       " 'cronobacter': 21,\n",
       " 'histamine': 22,\n",
       " 'bacteria': 23,\n",
       " 'other bacteria': 23,\n",
       " 'virus': 24,\n",
       " 'parasites': 25,\n",
       " 'carcinogen': 26,\n",
       " 'chemicals': 26,\n",
       " 'toxic': 26,\n",
       " 'toxin': 26,\n",
       " 'poisonous': 26,\n",
       " 'fungus': 26,\n",
       " 'food poisoning': 26,\n",
       " 'hazard': 26,\n",
       " 'efsa': 26,\n",
       " 'european food safety authority': 26,\n",
       " 'related terms': 26}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apriori_hazards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta_haz = create_eta(apriori_hazards, dictionary, 27)\n",
    "eta_prod = create_eta(apriori_products, dictionary, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow  = [dictionary.doc2bow(line) for line in corp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: GridSearchCV for each ! \n",
    "#TODO: I can add the multi-category terms into my seeded LDA model. maybe even give them lower coeffs than the more confident ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "197.32287216186523"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seconds_auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -7.58\n",
      "Topic 0: ['month', 'go', 'think', 'week', 'baby', 'use', 'know', 'like', 'get', 'good', 'try', 'help', 'one', 'time', 'take', 'say', 'start', 'need']\n",
      "Topic 1: ['get', 'go', 'take', 'hi', 'like', 'know', 'love', 'would', 'day', 'food', 'work', 'give', 'make', 'say', 'thing', 'time', 'eat', 'good']\n",
      "Topic 2: ['juice', 'shake', 'get', 'plus', 'plan', 'would', 'diet', 'eat', 'help', 'meal', 'hi', 'day', 'use', 'start', 'also', 'people', 'send', 'thanks']\n",
      "Topic 3: ['food', 'go', 'think', 'know', 'make', 'eat', 'old', 'want', 'give', 'good', 'year', 'help', 'would', 'son', 'get', 'really', 'day', 'month']\n",
      "Topic 4: ['baby', 'people', 'bottle', 'make', 'get', 'say', 'know', 'would', 'food', 'like', 'milk', 'take', 'old', 'cat', 'feed', 'use', 'give', 'thing']\n",
      "Topic 5: ['use', 'make', 'bottle', 'think', 'sent', 'netmums', 'really', 'need', 'milk', 'app', 'like', 'water', 'baby', 'mobile', 'day', 'feed', 'iphone', 'get']\n",
      "Topic 6: ['use', 'get', 'good', 'lol', 'day', 'would', 'like', 'say', 'make', 'app', 'mobile', 'really', 'go', 'well', 'think', 'sent', 'know', 'also']\n",
      "Topic 7: ['use', 'get', 'would', 'food', 'baby', 'milk', 'buy', 'time', 'make', 'bottle', 'help', 'day', 'netmums', 'eat', 'like', 'one', 'go', 'try']\n",
      "Topic 8: ['app', 'mobile', 'use', 'netmums', 'sent', 'iphone', 'send', 'go', 'sm', 'get', 'xx', 'would', 'like', 'gt', 'think', 'well', 'day', 'really']\n",
      "Topic 9: ['get', 'use', 'make', 'also', 'know', 'good', 'think', 'give', 'try', 'one', 'even', 'need', 'time', 'day', 'would', 'take', 'food', 'really']\n",
      "Topic 10: ['baby', 'milk', 'get', 'would', 'go', 'try', 'really', 'like', 'help', 'see', 'sleep', 'child', 'time', 'day', 'well', 'feel', 'night', 'take']\n",
      "Topic 11: ['get', 'go', 'emoji', 'think', 'day', 'good', 'work', 'really', 'one', 'like', 'find', 'say', 'eat', 'last', 'time', 'know', 'night', 'week']\n",
      "Topic 12: ['get', 'day', 'night', 'use', 'app', 'mobile', 'one', 'fruit', 'lol', 'today', 'make', 'gt', 'much', 'send', 'say', 'like', 'go', 'sent']\n",
      "Topic 13: ['get', 'use', 'baby', 'think', 'go', 'make', 'time', 'would', 'take', 'one', 'week', 'give', 'milk', 'need', 'really', 'like', 'know', 'say']\n",
      "Topic 14: ['buy', 'baby_food', 'food', 'would', 'get', 'yes', 'good', 'pouch', 'fruit', 'use', 'product', 'one', 'day', 'go', 'please', 'mamia', 'brand', 'aldi']\n",
      "Topic 15: ['like', 'get', 'make', 'week', 'good', 'try', 'day', 'go', 'use', 'thing', 'lose', 'think', 'bottle', 'would', 'lol', 'baby', 'eat', 'give']\n",
      "Topic 16: ['baby', 'aveeno', 'would', 'dermexa', 'range', 'skin', 'use', 'like', 'product', 'get', 'help', 'one', 'yes', 'cream', 'say', 'go', 'little', 'really']\n",
      "Topic 17: ['would', 'use', 'baby_food', 'one', 'like', 'get', 'yes', 'food', 'baby', 'mamia', 'cereal', 'go', 'buy', 'bran', 'range', 'good', 'purchase', 'know']\n",
      "Topic 18: ['say', 'go', 'get', 'milk', 'baby', 'eat', 'would', 'make', 'take', 'food', 'water', 'try', 'much', 'see', 'also', 'need', 'know', 'week']\n",
      "Topic 19: ['get', 'make', 'go', 'milk', 'think', 'one', 'need', 'use', 'day', 'water', 'really', 'eat', 'start', 'good', 'lose', 'give', 'try', 'like']\n",
      "Topic 20: ['think', 'one', 'get', 'use', 'dog', 'anyone', 'would', 'bath', 'go', 'make', 'know', 'bad', 'love', 'give', 'need', 'day', 'keep', 'time']\n",
      "Topic 21: ['get', 'go', 'back', 'water', 'try', 'month', 'also', 'use', 'child', 'eat', 'food', 'say', 'baby', 'would', 'give', 'make', 'need', 'one']\n",
      "Topic 22: ['use', 'make', 'get', 'take', 'day', 'go', 'app', 'one', 'mobile', 'bottle', 'know', 'would', 'netmums', 'water', 'need', 'send', 'thing', 'think']\n",
      "Topic 23: ['get', 'go', 'week', 'day', 'baby', 'use', 'say', 'good', 'like', 'eat', 'one', 'make', 'start', 'time', 'would', 'know', 'think', 'take']\n",
      "Topic 24: ['good', 'get', 'day', 'eat', 'well', 'go', 'walk', 'like', 'make', 'sal', 'work', 'today', 'really', 'food', 'hope', 'time', 'one', 'back']\n",
      "Topic 25: ['water', 'bottle', 'use', 'make', 'one', 'formula', 'boil', 'go', 'would', 'hot', 'get', 'add', 'milk', 'put', 'say', 'take', 'feed', 'hour']\n",
      "Topic 26: ['eat', 'meat', 'would', 'child', 'quorn', 'halal', 'make', 'use', 'think', 'know', 'good', 'get', 'feel', 'like', 'one', 'want', 'school', 'meal']\n"
     ]
    }
   ],
   "source": [
    "#model without Informed Priors\n",
    "start = time.time()\n",
    "test_eta('auto', dictionary, 27)\n",
    "end = time.time()\n",
    "seconds_auto = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "test_eta('auto', dictionary, 5)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model with Informed Priors\n",
    "\n",
    "start = time.time()\n",
    "test_eta(eta_prod, dictionary, 5)\n",
    "end = time.time()\n",
    "seconds_eta_p = end - start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_eta_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: -1.13\n",
      "Topic 0: ['month', 'go', 'think', 'week', 'baby', 'use', 'like', 'know', 'get', 'good', 'try', 'help', 'one', 'time', 'say', 'want', 'take', 'need']\n",
      "Topic 1: ['get', 'go', 'take', 'hi', 'like', 'food', 'know', 'would', 'day', 'eat', 'give', 'say', 'love', 'time', 'work', 'way', 'make', 'think']\n",
      "Topic 2: ['shake', 'juice', 'get', 'plus', 'plan', 'would', 'help', 'diet', 'eat', 'meal', 'day', 'hi', 'start', 'use', 'weight', 'thanks', 'also', 'rep']\n",
      "Topic 3: ['food', 'go', 'think', 'eat', 'know', 'make', 'want', 'old', 'give', 'get', 'day', 'year', 'good', 'really', 'would', 'help', 'son', 'like']\n",
      "Topic 4: ['baby', 'people', 'bottle', 'make', 'say', 'get', 'know', 'cat', 'take', 'would', 'like', 'feed', 'old', 'food', 'go', 'need', 'use', 'milk']\n",
      "Topic 5: ['use', 'make', 'think', 'bottle', 'sent', 'really', 'need', 'netmums', 'milk', 'water', 'app', 'like', 'day', 'get', 'mobile', 'baby', 'try', 'much']\n",
      "Topic 6: ['use', 'get', 'good', 'lol', 'like', 'day', 'would', 'say', 'make', 'app', 'go', 'mobile', 'really', 'well', 'sent', 'bath', 'also', 'think']\n",
      "Topic 7: ['use', 'get', 'would', 'food', 'baby', 'milk', 'buy', 'time', 'eat', 'make', 'day', 'help', 'bottle', 'go', 'netmums', 'one', 'try', 'like']\n",
      "Topic 8: ['use', 'app', 'mobile', 'netmums', 'sent', 'iphone', 'send', 'sm', 'go', 'get', 'like', 'would', 'xx', 'think', 'gt', 'day', 'say', 'well']\n",
      "Topic 9: ['get', 'use', 'make', 'good', 'give', 'also', 'think', 'one', 'know', 'take', 'try', 'need', 'even', 'day', 'food', 'would', 'time', 'app']\n",
      "Topic 10: ['get', 'baby', 'milk', 'would', 'go', 'try', 'really', 'sleep', 'help', 'like', 'see', 'day', 'child', 'time', 'well', 'night', 'take', 'feel']\n",
      "Topic 11: ['get', 'go', 'emoji', 'day', 'good', 'think', 'work', 'really', 'like', 'one', 'last', 'eat', 'night', 'find', 'time', 'know', 'say', 'week']\n",
      "Topic 12: ['get', 'night', 'use', 'fruit', 'day', 'one', 'make', 'try', 'much', 'today', 'say', 'water', 'like', 'go', 'mobile', 'app', 'take', 'lol']\n",
      "Topic 13: ['get', 'use', 'think', 'baby', 'go', 'time', 'make', 'would', 'week', 'take', 'one', 'give', 'milk', 'need', 'really', 'like', 'know', 'feel']\n",
      "Topic 14: ['buy', 'baby_food', 'would', 'food', 'yes', 'get', 'pouch', 'product', 'use', 'fruit', 'good', 'one', 'mamia', 'please', 'go', 'day', 'aldi', 'brand']\n",
      "Topic 15: ['like', 'make', 'get', 'week', 'good', 'try', 'thing', 'go', 'day', 'use', 'lol', 'would', 'eat', 'think', 'really', 'lose', 'give', 'baby']\n",
      "Topic 16: ['baby', 'aveeno', 'would', 'dermexa', 'use', 'skin', 'range', 'like', 'get', 'product', 'one', 'help', 'yes', 'cream', 'say', 'day', 'little', 'time']\n",
      "Topic 17: ['would', 'use', 'one', 'baby_food', 'like', 'get', 'yes', 'food', 'baby', 'cereal', 'mamia', 'go', 'buy', 'bran', 'range', 'good', 'purchase', 'know']\n",
      "Topic 18: ['say', 'milk', 'go', 'get', 'baby', 'eat', 'would', 'take', 'make', 'food', 'low', 'need', 'see', 'try', 'child', 'much', 'also', 'give']\n",
      "Topic 19: ['get', 'make', 'go', 'milk', 'think', 'one', 'need', 'use', 'eat', 'water', 'day', 'really', 'lose', 'try', 'cheese', 'start', 'like', 'good']\n",
      "Topic 20: ['think', 'one', 'use', 'get', 'bath', 'anyone', 'go', 'make', 'give', 'know', 'day', 'would', 'time', 'eat', 'food', 'bad', 'well', 'say']\n",
      "Topic 21: ['get', 'back', 'go', 'water', 'month', 'use', 'also', 'try', 'eat', 'child', 'would', 'much', 'think', 'baby', 'say', 'food', 'make', 'need']\n",
      "Topic 22: ['use', 'get', 'make', 'app', 'take', 'go', 'mobile', 'day', 'netmums', 'one', 'would', 'know', 'xx', 'send', 'bad', 'sent', 'need', 'water']\n",
      "Topic 23: ['get', 'go', 'week', 'use', 'day', 'baby', 'say', 'good', 'make', 'one', 'like', 'time', 'start', 'eat', 'would', 'know', 'think', 'also']\n",
      "Topic 24: ['good', 'day', 'get', 'eat', 'well', 'go', 'walk', 'like', 'make', 'food', 'really', 'time', 'hope', 'sal', 'one', 'dog', 'hi', 'work']\n",
      "Topic 25: ['water', 'bottle', 'use', 'make', 'formula', 'one', 'boil', 'would', 'go', 'hot', 'get', 'milk', 'put', 'add', 'say', 'take', 'feed', 'baby']\n",
      "Topic 26: ['eat', 'meat', 'would', 'child', 'halal', 'quorn', 'use', 'make', 'think', 'get', 'know', 'good', 'feel', 'want', 'school', 'one', 'like', 'meal']\n"
     ]
    }
   ],
   "source": [
    "#model with Informed Priors\n",
    "\n",
    "start = time.time()\n",
    "test_eta(eta_haz, dictionary, 27)\n",
    "end = time.time()\n",
    "seconds_eta_h = end - start\n",
    "#it runs faster after changing passes to a low number (3) and update to 0 (batch instead of online.)\n",
    "#I can tweak it later on i guess.\n",
    "#But now I know it doent have to be inhumanly slow! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(seconds_auto, seconds_eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: simplify the creation of eta according to the documentation so its just based on dictionary\n",
    "#TODO: check for convergence to see if pass # is correctly specified.\n",
    "#TODO: set up grid searchCV\n",
    "\n",
    "\n",
    "#eta ({float, np.array, str}, optional) –\n",
    "#\n",
    "#A-priori belief on word probability, this can be:\n",
    "#\n",
    "#        scalar for a symmetric prior over topic/word probability,\n",
    "#\n",
    "#        vector of length num_words to denote an asymmetric user defined probability for each word,\n",
    "#\n",
    "#        matrix of shape (num_topics, num_words) to assign a probability for each word-topic combination,\n",
    "#         ^ this is the one the guy chose.\n",
    "#        the string ‘auto’ to learn the asymmetric prior from the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stats.stackexchange.com/questions/74487/topic-models-evaluation-in-gensim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Search Param\n",
    "search_params = {'n_components': [10, 15, 20, 25, 30], 'learning_decay': [.5, .7, .9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/60602768/scikit-learn-gridsearchcv-failing-on-on-a-gensim-lda-model\n",
    "# https://github.com/RaRe-Technologies/gensim/blob/a83e61b768d53ab3bab72abe4aa7db9aab66593c/docs/notebooks/sklearn_wrapper.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) build grid search\n",
    "2) add in cross validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRidSearcg:\n",
    "- dictionary of all parameters.\n",
    "- generate and evaluate each model one by one\n",
    "- save model and results into a dict i guess\n",
    "- save results into dict as well.\n",
    "- show the best perfoming model from each.\n",
    "https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/topic_coherence_model_selection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # set the random seed for repeatability\n",
    "\n",
    "bow = [dictionary.doc2bow(line) for line in corp] # get the bow-format lines with the set dictionary\n",
    "\n",
    "with (np.errstate(divide='ignore')):  # ignore divide-by-zero warnings\n",
    "    informed_model = gensim.models.ldamodel.LdaModel(\n",
    "        corpus=bow, id2word=dictionary, num_topics=ntopics,\n",
    "        random_state=42, chunksize=100, eta=eta,\n",
    "        eval_every=-1, update_every=0, #0 is batch, 1 is online.\n",
    "        passes=10, alpha='auto', per_word_topics=False) #I LOWERED THE PASSES BY A LOT TO BE FASTER, hEh..\n",
    "    normal_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init the Model\n",
    "#lda = LatentDirichletAllocation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Init Grid Search Class\n",
    "#model = GridSearchCV(lda, param_grid=search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Do the Grid Search\n",
    "#model.fit(data_vectorized)\n",
    "#\n",
    "#\n",
    "#GridSearchCV(cv=None, error_score='raise',\n",
    "#       estimator=LatentDirichletAllocation(batch_size=128, doc_topic_prior=None,\n",
    "#             evaluate_every=-1, learning_decay=0.7, learning_method=None,\n",
    "#             learning_offset=10.0, max_doc_update_iter=100, max_iter=10,\n",
    "#             mean_change_tol=0.001, n_components=10, n_jobs=1,\n",
    "#             n_topics=None, perp_tol=0.1, random_state=None,\n",
    "#             topic_word_prior=None, total_samples=1000000.0, verbose=0),\n",
    "#       fit_params=None, iid=True, n_jobs=1,\n",
    "#       param_grid={'n_topics': [10, 15, 20, 25, 30], 'learning_decay': [0.5, 0.7, 0.9]},\n",
    "#       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
    "#       scoring=None, verbose=0)\n",
    "#\t   \n",
    "## Best Model\n",
    "#best_lda_model = model.best_estimator_\n",
    "#\n",
    "## Model Parameters\n",
    "#print(\"Best Model's Params: \", model.best_params_)\n",
    "#\n",
    "## Log Likelihood Score\n",
    "#print(\"Best Log Likelihood Score: \", model.best_score_)\n",
    "#\n",
    "## Perplexity\n",
    "#print(\"Model Perplexity: \", best_lda_model.perplexity(data_vectorized))\n",
    "#\t   \n",
    "## Get Log Likelyhoods from Grid Search Output\n",
    "#n_topics = [10, 15, 20, 25, 30]\n",
    "#log_likelyhoods_5 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.5]\n",
    "#log_likelyhoods_7 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.7]\n",
    "#log_likelyhoods_9 = [round(gscore.mean_validation_score) for gscore in model.grid_scores_ if gscore.parameters['learning_decay']==0.9]\n",
    "#\n",
    "## Show graph\n",
    "#plt.figure(figsize=(12, 8))\n",
    "#plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "#plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "#plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "#plt.title(\"Choosing Optimal LDA Model\")\n",
    "#plt.xlabel(\"Num Topics\")\n",
    "#plt.ylabel(\"Log Likelyhood Scores\")\n",
    "#plt.legend(title='Learning decay', loc='best')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
