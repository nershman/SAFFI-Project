{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we:\n",
    "* select a subset based on indicators derived from the text and related data\n",
    "    * we do not process the text as thorougly at this step because we are only working with entire threads at this point, so it should be expected that the vocabulary we are searching for appear at least once in each thread.\n",
    "* process text so that it can be better evaluated in further steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if jupyternotify is installed, we can add %notify to a cell to get an alert when it ifnished running\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics_helpers as indicators\n",
    "import pickle as pk\n",
    "import gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_to_int(dt): #datetime to integer\n",
    "    return dt.astype('int')/(10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#netmums\n",
    "\n",
    "\n",
    "with open('/Users/sma/Documents/INRAE internship/scrape-git/netmums/allposts_rerun.pkl', 'rb') as f:\n",
    "    netmums = pk.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Export the Keys for our Desired Subset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Text\n",
    "We now construct our subset form the desired keys and then process the text.\n",
    "* TODO: clean the text before we run it through the next steps. By removing hyphens, upper cases, etc.\n",
    "    * but not lemmatization, unless we also lemmatize our lists of words to search for!!!\n",
    "\n",
    "* remove typos of relevant words using Levenshtein Distances\n",
    "* replace tokens for specific foods and brands with their category, after compiling lists of these terms using word2vec\n",
    "    * replace tokens for all types of fruits with fruit\n",
    "    * replace tokens for all types of vegetable with vegetable\n",
    "    * replace tokens for all types of grains with \"cereal\" (???) should I??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Define Cleaning Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note if we replace hyphens with spaces at this step we may have\n",
    "#issues fully removing URLs. Let's remove hyphens later in the pipeline.\n",
    "\n",
    "#lowercasing is implemented as an option within the package.\n",
    "import re #TODO: is there a better way of doing this? my pakage already imports re.\n",
    "def clean(text):\n",
    "    #lowercase\n",
    "    text = text.lower()\n",
    "    #remove URLs.\n",
    "    reg = '\\S+.(?:co|net|tv|org|edu|gov)\\S*'\n",
    "    text = re.sub(reg, '', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Create Lists for Relevant Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "baby_formula = \\\n",
    "['nutramigen',\n",
    " 'neocate',\n",
    " 'powdered milk',\n",
    " 'infasoy',\n",
    " 'comfort milk', #brand name which people dont write formula alongside\n",
    " 'sma' \n",
    "]\n",
    "\n",
    "baby_cereal = \\\n",
    "['baby rice '#this one is really useful / important. idk how exactly to handle it.\n",
    "'rusks' #a cereal food for babies to teethe with\n",
    "]\n",
    "\n",
    "cereal = \\\n",
    "['cornflakes',\n",
    "'muesli',\n",
    "'bran flakes',\n",
    "'cheerios',\n",
    "'shreddies',\n",
    "'weetabix',\n",
    "'ready brek',\n",
    "'rice pudding',\n",
    "'rice'\n",
    "]\n",
    "\n",
    "fruit = \\\n",
    "['banana',\n",
    "'berries',\n",
    "'blueberries',\n",
    "'raisins',\n",
    "'apples',\n",
    "'pear',\n",
    "'strawberries',\n",
    "'pineapple', \n",
    "'raspberries',\n",
    "'mango', \n",
    "'prunes', \n",
    "'grapefruit']\n",
    "\n",
    "veg = \\\n",
    "['mushroom', \n",
    "'red_pepper',\n",
    "'green_beans', \n",
    "'courgette', \n",
    "'broccoli', \n",
    "'tomato',\n",
    "'parsnips', \n",
    "'greens', \n",
    "'potato', \n",
    "'carrots',\n",
    "'broccoli',\n",
    "'cucumber', \n",
    "'peas', \n",
    "'tomatoes', \n",
    "'sweet_potato',\n",
    "'sweetcorn', \n",
    "'corn', \n",
    "'spinach', \n",
    "'cauliflower',\n",
    "'butternut squash', \n",
    "'beetroot',\n",
    "'squash']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "foodwords = [\n",
    "#infant formula\n",
    "\"formula\",\"baby formula\", \"bottle-fed\", \"bottle\",\n",
    "#sterizlized vegetable mixed with fish\n",
    "\"veggie baby food\",\"vegetable baby food\",\n",
    "\"veg puree\", \"veg purée\",\n",
    "#fresh fruit puree mildly processed\n",
    "\"fruit puree\",\"fruit baby food\", \"fruit purée\", \"applesauce\",\n",
    "#infant cereals\n",
    "\"cereal for baby\", \"cereal\", \"porridge\", \"oats\", \"oatmeal\",\n",
    "#other\n",
    "\"jar food\", \"baby food\", \"jarred\", \"premade food\", \"puree\", \"purée\", \"jarred food\"\n",
    ",\"yoghurt\", \"pudding\"]\n",
    "\n",
    "hazardwords = [\"Chemical contaminants\",#ENDOCRINE DISRUPTOR\n",
    "\"Endocrine disruptor\",\"endocrine\",\"estrogen\",#end\n",
    "#FOOD PRESERVATIVES, SWEETENERS AND ADDITIVES\n",
    "\"preservatives\",\"sweeteners\",\"additives\", #end\n",
    "\"Pesticides\",#VETERINARY DRUGS\n",
    "\"Veterinary drugs\",\"animal drugs\",\"vet drugs\", #end\n",
    "#GMO\n",
    "\"GMO\", \"genetically modified\",#end\n",
    "\"Metals\",\"Mycotoxin\",#BISPHENOL A\n",
    "\"Bisphenol\",\"BPA\", #end\n",
    "#FURAN - removed because nothing related to this returns results\n",
    "#DON (note that this acronym nobody uses and all results are from words like \"don't\")\n",
    "\"deoxynivalenol\",\"vomitoxin\",#end\n",
    "#DIOXIN AND PCB\n",
    "\"Dioxin\",\"PCB\",\"biphenyls\",#end\n",
    "#MOH\n",
    "\"MOH\",\"hydrocarbons\",\"saturated hydrocarbons\",\"MOAH\",\"aromatic hydrocarbons\",#end\n",
    "\"Nitrates\",\n",
    "#ACRYLAMID\n",
    "\"Acrylamide\",\n",
    "\"phthalates\",\n",
    "#MICROBIOLOGIC CONTAMINANTS\n",
    "\"Microbiologic contaminants\",\"spores\",\"mold\",\"mould\",\"virus\",\"microbes\",\"contaminated\",#end\n",
    "\"Salmonella\",\"Campylobacter\",\"Listeria\",\n",
    "#ECOLI\n",
    "\"EColi\",\n",
    "\"Cronobacter\",\n",
    "\"Histamine\",\n",
    "#other bacteria\n",
    "\"bacteria\",#end\n",
    "\"Virus\",\n",
    "\"Parasites\",\n",
    "#UNRELATED BUT MAYBE USEFUL?\n",
    "\"carcinogen\",\"chemicals\", \"toxic\", \"toxin\", \"poisonous\", \"fungus\", \"food poisoning\", \"hazard\",\"EFSA\",\"European Food Safety Authority\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fuzzy_typos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "typos_to_fix_or_replace = veg + fruit + cereal + baby_formula + baby_cereal\n",
    "typos_to_fix_or_replace = {word for phrase in typos_to_fix_or_replace for word in phrase.split()} #typos to fix and single tokens to replace\n",
    "\n",
    "replacements_dictionary = {'vegetable':veg, 'fruit':fruit, 'cereal':cereal, 'baby cereal':baby_cereal, 'baby formula': baby_formula}\n",
    "\n",
    "remaining_words_to_replace = {key:[item for item in value if ' ' in item] for key, value in replacements_dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_and_replace_tokens = fuzzy_typos.fuzzy_typos(typos_to_fix_or_replace, replacements_dictionary, cleaner = clean)\n",
    "replace_phrases = fuzzy_typos.replacements(remaining_words_to_replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup: Parallel Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = list(netmums.keys())\n",
    "num_keys = len(keys)\n",
    "num_lists = 20 #how many instances will be split for parallel processing\n",
    "list_of_list_of_keys = [keys[slice(i,num_keys,num_lists)] for i in range(num_lists)]\n",
    "\n",
    "def get_small_dict(list_of_keys): #we give process small dicts because o.w. the whole dict (a global) will get duplicated in each instance\n",
    "    return {key: netmums[key] for key in list_of_keys}\n",
    "\n",
    "def process(typofixer,replacer,small_dict): #now process takes two objects\n",
    "    #approx 1.5x slower than the text_dict way.\n",
    "    #THE RELEVANT THINGS:\n",
    "    #netmums[blah]['title']\n",
    "    #netmums[blah]['posts'][n]['body']\n",
    "    #netmums[blah]['posts'][n]['quotes_w']\n",
    "    #netmums[blah]['posts'][n]['quotes_y']['text']\n",
    "    for key, value in small_dict.items():\n",
    "        small_dict[key]['title'] = typofixer.fix_typos(small_dict[key]['title'])\n",
    "        for ind, item in enumerate(small_dict[key]['posts']): #a list of dicts\n",
    "            if item['body']:\n",
    "                small_dict[key]['posts'][ind]['body'] = replacer.replace_all(typofixer.fix_typos(item['body']))\n",
    "            if item['quotes_w']:\n",
    "                for qind, quote in enumerate(item['quotes_w']):\n",
    "                    small_dict[key]['posts'][ind]['quotes_w'][qind] = replacer.replace_all(typofixer.fix_typos(quote))\n",
    "            if item['quotes_y']:\n",
    "                for qind, quote in enumerate(item['quotes_y']):\n",
    "                    small_dict[key]['posts'][ind]['quotes_y'][qind]['text'] = replacer.replace_all(typofixer.fix_typos(quote['text']))     \n",
    "    return small_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate dicts which we will feed into the parallel processing\n",
    "#if we feed the entire dict in and generate them from within it, \n",
    "#the whole dict will get duplicated  many times wasting memory.\n",
    "\n",
    "list_of_small_dict = [get_small_dict(i) for i in list_of_list_of_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally Running It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "default food words time: 3105.523297071457\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"6a683abf-9db9-40c5-8c35-372499e59dbb\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"6a683abf-9db9-40c5-8c35-372499e59dbb\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start = time.time()\n",
    "results = Parallel(n_jobs=-1)(delayed(process)(fix_and_replace_tokens,replace_phrases,i) for i in list_of_small_dict)\n",
    "end = time.time()\n",
    "print('default food words time: ' + str(end - start),)\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "untypod_dict = {key:value for dictionary in results for key,value in dictionary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('FULL_untypod_dict.pkl', 'wb') as f:\n",
    "    pk.dump(untypod_dict, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
