{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses the same cleaning routine as NETMUMS_topicmining_POSTS.\n",
    "It is used to see what words co-occur with haza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#if jupyternotify is installed, we can add %notify to a cell to get an alert when it ifnished running\n",
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics_helpers as indicators\n",
    "import pickle as pk\n",
    "import gc\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import traceback #needed to store full error tracebacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_to_int(dt): #datetime to integer\n",
    "    return dt.astype('int')/(10**9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/sma/Documents/INRAE internship/scrape-git/facebook/untypod_dict.pkl', 'rb') as f:\n",
    "    netmums = pk.load(f)\n",
    "\n",
    "#with open('/Users/sma/Documents/INRAE internship/scrape-git/netmums/allposts_rerun.pkl', 'rb') as f:\n",
    "#    netmums = pk.load(f)\n",
    "    \n",
    "#with open('/Users/sma/Documents/INRAE internship/scrape-git/netmums/netmums_subset_keys.txt', 'r') as f:\n",
    "#    keys = [url.strip() for url in f.readlines()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nm_ind = indicators.indicators(netmums, fb=False)\n",
    "#this one takes long, around 20 seconds I think.\n",
    "\n",
    "posts_dict = nm_ind.get_posts_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hazards = {\n",
    "'Chemical contaminants': [],\n",
    "'Endocrine disruptor': [\"endocrine\",\"estrogen\"],\n",
    "'FOOD PRESERVATIVES, SWEETENERS AND ADDITIVES':[\"preservatives\",\"sweeteners\",\"additives\"],\n",
    "\"Pesticides\":[],\n",
    "\"Veterinary drugs\":[\"animal drugs\",\"vet drugs\"],\n",
    "'GMO':['GM',\"genetically modified\"],\n",
    "\"Metals\":[],\n",
    "\"Mycotoxin\":[],\n",
    "\"Bisphenol A\":['BPA','Bisphenol','BisphenolA'],\n",
    "'Furan':[],\n",
    "'DON': #(note that this acronym nobody uses and all results are from words like \"don't\")\n",
    "[\"deoxynivalenol\",\n",
    "\"vomitoxin\"],\n",
    "'DIOXIN AND PCB':[\"Dioxin\",\"PCB\",\"biphenyls\"],\n",
    "'MOSH and MOAH':[\"hydrocarbons\",\"saturated hydrocarbons\",\"MOAH\", 'MOH',\"aromatic hydrocarbons\"],\n",
    "'Nitrates':[],\n",
    "\"Acrylamid\":[\"Acrylamide\"],\n",
    "\"phthalates\":[],\n",
    "\"Microbiologic contaminants\":\n",
    "[\"spores\",\n",
    "\"mold\",\n",
    "\"mould\",\n",
    "#\"virus\",\n",
    "\"microbes\",\n",
    "\"contaminated\"],\n",
    "\"Salmonella\":[],\n",
    "\"Campylobacter\":[],\n",
    "\"Listeria\":[],\n",
    "\"EColi\":[\"E-coli\"],\n",
    "\"Cronobacter\":[],\n",
    "\"Histamine\":[],\n",
    "'other bacteria':[\"bacteria\"],\n",
    "\"Virus\":[],\n",
    "\"Parasites\":[],\n",
    "'Related Terms':[\"carcinogen\",\"chemicals\", \"toxic\", \"toxin\", \"poisonous\", \"fungus\", \"food poisoning\", \"hazard\",\"EFSA\",\"European Food Safety Authority\"]\n",
    "}\n",
    "\n",
    "products = {\n",
    "'infant formula':\n",
    "[\"formula\",\"baby formula\", \"bottle-fed\", \"bottle\"]\n",
    ",'sterilized vegetable mixed with fish':\n",
    "[\"veggie baby food\",\"vegetable baby food\",\n",
    "\"veg puree\", \"veg purée\"]\n",
    ",'fresh fruit puree mildly processed':\n",
    "[\"fruit puree\",\"fruit baby food\", \"fruit purée\", \"applesauce\", \"apple sauce\", \"fruit sauce\"]\n",
    ",'infant cereals':\n",
    "[\"cereal for baby\", \"cereal\", \"porridge\", \"oats\", \"oatmeal\"]\n",
    ",'other':\n",
    "[\"jar food\", \"baby food\", \"jarred\", \"premade food\", \"puree\", \"purée\", \"jarred food\"\n",
    ",\"yoghurt\", \"pudding\"]\n",
    "}\n",
    "\n",
    "\n",
    "#IMPORTANT!: terms used for count vectorizer must be lower-case o.w. get 0 matches\n",
    "hazards = {key.lower():[v.lower() for v in value] + [key.lower()] for key,value in hazards.items()}\n",
    "products = {key.lower():[v.lower() for v in value]+[key.lower()] for key,value in products.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "extras = {\\\n",
    "'baby_food_brands':\n",
    "['ellas',\n",
    "'organix',\n",
    "'heinz baby',\n",
    "\"plum baby\",\n",
    "'little angels',\n",
    "'farleys'],\n",
    "'formula_brands':['sma','aptamil comfort','infasoy','nutramigen','neocate','powdered milk','comfort milk'],\n",
    " 'food_or_formula_brands':\n",
    "['aptamil', # formula and cereals.\n",
    "'hipp organic',# - formula and baby food\n",
    "'cow gate','cow and gate','c g',\n",
    "'mamia'],\n",
    "##NON BRAND SIGNALS##\n",
    "'cereal':['baby_cereal','baby riceporridge','baby rice','baby porridge'],\n",
    "'baby_food':['mashed','tinned','premade','canned','jarred','pouches','pouch','ready made','readymade','cartons'],  \n",
    "#INDICATORS TO BE USED IN CONJUNCTION WITH 'baby food' label: this way we \n",
    "#can observe if both terms are used in a document (but are not used right next to each other.)\n",
    "'fruit':['fruit'],\n",
    "'vegetable':['vegetable'],\n",
    "'baby':['infant', 'baby' ,'for littles']\n",
    "         }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def make_phrases(list_of_phrases, text):\n",
    "    \"\"\"\n",
    "    convert phrases to bigrams within a larger text corpus.\n",
    "    example: \"I love collard greens for breakfast\" -> \"I love collard_greens for breakfast\"\n",
    "    example: \"I love collard-greens for breakfast\" -> \"I love collard_greens for breakfast\"\n",
    "    \"\"\"\n",
    "    for phrase in list_of_phrases:\n",
    "        #spaces\n",
    "        text = re.sub(phrase, re.sub(' ', '_',phrase), text)\n",
    "        #hyphens\n",
    "        text = re.sub(re.sub(' ', '-', phrase), re.sub(' ', '_',phrase), text)\n",
    "    return text\n",
    "\n",
    "def make_underscores(item):\n",
    "    \"\"\"\n",
    "    recursively replace spaces and hyphens in strings, lists, sets, or other iterables.\n",
    "    Return the same type if string, list, set. If other type, returns list.\n",
    "    \"\"\"\n",
    "    if type(item) is str:\n",
    "        return re.sub(' |-', '_', item)\n",
    "    else:\n",
    "        temp = []\n",
    "        for thing in item:\n",
    "            temp.append(make_underscores(thing))\n",
    "    if type(item) is set:\n",
    "        return set(temp)\n",
    "    elif type(item) is list:\n",
    "        return temp\n",
    "    elif isinstance(item, type({}.keys())):\n",
    "        #if the object is a dict.key() view\n",
    "        return temp\n",
    "    else:\n",
    "        print('Object must be string, list, set, or dict.keys()')\n",
    "    #TODO this would be cleaner if i just check that it's iterable, and then check that it's a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from the dict which representes our subcategories, create lists of all words in the subcategories.\n",
    "h = [item for val in hazards.values() for item in val]\n",
    "p = [item for val in products.values() for item in val]\n",
    "e = [item for val in extras.values() for item in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "#concatenate list of all phrases (bigrams, anything with a space in it)\n",
    "phrases = {'baby formula', 'baby cereal'}.union({item for item in p + h + e if ' ' in item})\n",
    "\n",
    "#step 1: make a dict of just the text\n",
    "text_dict = {key:value['body'] for key,value in posts_dict.items()}\n",
    "\n",
    "#step 2 : convert the relevant phrases to bigrams with re.sub\n",
    "text_dict = {key: make_phrases(phrases, text) for key, text in text_dict.items()}\n",
    "\n",
    "#replace \"don't\" with \"do not\" (so that we don't get false positives for don count.)\n",
    "for key in text_dict:\n",
    "    text_dict[key] = re.sub('don[\\W]+t', 'do not', text_dict[key], flags=re.I) #TODO. there are cases of \"don' \" need to catch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GenSim / Word2Vec Implementation on Threads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec, Phrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can try running the model on only our subset and see how long it takes.\n",
    "\n",
    "Then maybe we can run it on the entire dataset.\n",
    "\n",
    "The end goal of this is to determine which words are related to ngrams like \"baby food\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Process our data to the right format for feeding into the model\n",
    "keys = list(nm_ind.text_dict.keys())\n",
    "text_list = list(nm_ind.text_dict.values())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define functions\n",
    "def deEmojify(text):\n",
    "    #remove emoji (FIXME: doesnt remove all of them.)\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)\n",
    "\n",
    "def clean(text):\n",
    "    #remove URLs\n",
    "    pdf_regex='http[\\S]+pdf[\\S]*'\n",
    "    regex = r'http\\S+'\n",
    "    text = re.sub(regex, 'urlpostedtopdf', text)\n",
    "    text = re.sub(regex, 'urlpostedtosomething', text)\n",
    "    #TODO: remove emails\n",
    "\n",
    "    #replace commas and semicolons with spaces.\n",
    "    text = re.sub('[;,&\\+]+', ' ', text)\n",
    "    #remove hyphens\n",
    "    text = re.sub('[-]+', ' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEAN TEXT\n",
    "text_list = [deEmojify(i) for i in text_list]\n",
    "\n",
    "text_list = [clean(item) for item in text_list]\n",
    "\n",
    "\n",
    "#split sentences, new lines\n",
    "text_list = [j for i in text_list for j in re.split('[\\n?!.]+', i)]\n",
    "\n",
    "#remove empty items\n",
    "text_list = [i for i in text_list if i]\n",
    "#strip reamining elements from text\n",
    "text_list = [re.sub(r'[^A-Za-z0-9 ]+', '', i) for i in text_list]\n",
    "#remove extra spaces\n",
    "text_list = [re.sub(r'\\s+', ' ', i) for i in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to transform list of sentence to LoL of words\n",
    "tokens = [list(tokenize(doc, lower=True)) for doc in text_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"a1eb705c-dc3e-4e17-a1b9-214a56536763\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"a1eb705c-dc3e-4e17-a1b9-214a56536763\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train a bigram detector.\n",
    "#this detects bigrams and converts them to single tokens simply by relplacing space w underscore\n",
    "#check the google paper for more info on how it's done.\n",
    "#https://datascience.stackexchange.com/questions/25524/how-does-phrases-in-gensim-work\n",
    "\n",
    "#Detect phrases based on collocation counts.\n",
    "bigram_transformer = Phrases(tokens, threshold = 2, connector_words=phrases.ENGLISH_CONNECTOR_WORDS)\n",
    "#moving the threshhold a bit higher.. about 12, will get rid of a lot\n",
    "#of non-phrase bigrams (what_brand, an_email) but I'm not sure how this would be beneficial.\n",
    "#TODO: read how the word similarity is calculated.\n",
    "\n",
    "# Apply the trained MWE detector to a corpus, using the result to train a Word2vec model.\n",
    "model = Word2Vec(bigram_transformer[tokens], min_count=1)\n",
    "\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram_transformer = Phrases(tokens, threshold = 10) \n",
    "temp = sorted(bigram_transformer.export_phrases().items(), key= lambda x:x[1] * -1 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
